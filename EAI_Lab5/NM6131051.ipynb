{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KRm0_WztAPe"
   },
   "source": [
    "# 2025 EAI Lab 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e7m9tHTpR0H"
   },
   "source": [
    "## Topic 1 : From PyTorch To ONNX\n",
    "\n",
    "### Steps:\n",
    "1.   Define Model Architecture\n",
    "2.   Load Weight\n",
    "3.   Export ONNX File\n",
    "4.   Quantize To INT8\n",
    "5.   Building Session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vt7LM45spK0Q"
   },
   "outputs": [],
   "source": [
    "# !pip install -U \\\n",
    "#     torch torchvision torchaudio \\\n",
    "#     onnx onnxscript onnxruntime onnxruntime-tools onnxruntime-gpu \\\n",
    "#     gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MCKZlTEIqkOD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TODO\n",
    "# Design Your ResNet18 Model\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "                    nn.BatchNorm2d(out_channels),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                    nn.BatchNorm2d(out_channels)\n",
    "                )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, s))\n",
    "            self.in_channels = out_channels \n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        \n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        \n",
    "        out = out.view(out.size(0), -1) \n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RUErZRINpUU5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> best_model.pth\n",
      "checkpoint keys: odict_keys(['total_ops', 'total_params', 'conv1.0.weight', 'conv1.1.weight', 'conv1.1.bias', 'conv1.1.running_mean', 'conv1.1.running_var', 'conv1.1.num_batches_tracked', 'layer1.0.total_ops', 'layer1.0.total_params', 'layer1.0.left.0.weight', 'layer1.0.left.1.weight', 'layer1.0.left.1.bias', 'layer1.0.left.1.running_mean', 'layer1.0.left.1.running_var', 'layer1.0.left.1.num_batches_tracked', 'layer1.0.left.3.weight', 'layer1.0.left.4.weight', 'layer1.0.left.4.bias', 'layer1.0.left.4.running_mean', 'layer1.0.left.4.running_var', 'layer1.0.left.4.num_batches_tracked', 'layer1.1.total_ops', 'layer1.1.total_params', 'layer1.1.left.0.weight', 'layer1.1.left.1.weight', 'layer1.1.left.1.bias', 'layer1.1.left.1.running_mean', 'layer1.1.left.1.running_var', 'layer1.1.left.1.num_batches_tracked', 'layer1.1.left.3.weight', 'layer1.1.left.4.weight', 'layer1.1.left.4.bias', 'layer1.1.left.4.running_mean', 'layer1.1.left.4.running_var', 'layer1.1.left.4.num_batches_tracked', 'layer2.0.total_ops', 'layer2.0.total_params', 'layer2.0.left.0.weight', 'layer2.0.left.1.weight', 'layer2.0.left.1.bias', 'layer2.0.left.1.running_mean', 'layer2.0.left.1.running_var', 'layer2.0.left.1.num_batches_tracked', 'layer2.0.left.3.weight', 'layer2.0.left.4.weight', 'layer2.0.left.4.bias', 'layer2.0.left.4.running_mean', 'layer2.0.left.4.running_var', 'layer2.0.left.4.num_batches_tracked', 'layer2.0.shortcut.0.weight', 'layer2.0.shortcut.1.weight', 'layer2.0.shortcut.1.bias', 'layer2.0.shortcut.1.running_mean', 'layer2.0.shortcut.1.running_var', 'layer2.0.shortcut.1.num_batches_tracked', 'layer2.1.total_ops', 'layer2.1.total_params', 'layer2.1.left.0.weight', 'layer2.1.left.1.weight', 'layer2.1.left.1.bias', 'layer2.1.left.1.running_mean', 'layer2.1.left.1.running_var', 'layer2.1.left.1.num_batches_tracked', 'layer2.1.left.3.weight', 'layer2.1.left.4.weight', 'layer2.1.left.4.bias', 'layer2.1.left.4.running_mean', 'layer2.1.left.4.running_var', 'layer2.1.left.4.num_batches_tracked', 'layer3.0.total_ops', 'layer3.0.total_params', 'layer3.0.left.0.weight', 'layer3.0.left.1.weight', 'layer3.0.left.1.bias', 'layer3.0.left.1.running_mean', 'layer3.0.left.1.running_var', 'layer3.0.left.1.num_batches_tracked', 'layer3.0.left.3.weight', 'layer3.0.left.4.weight', 'layer3.0.left.4.bias', 'layer3.0.left.4.running_mean', 'layer3.0.left.4.running_var', 'layer3.0.left.4.num_batches_tracked', 'layer3.0.shortcut.0.weight', 'layer3.0.shortcut.1.weight', 'layer3.0.shortcut.1.bias', 'layer3.0.shortcut.1.running_mean', 'layer3.0.shortcut.1.running_var', 'layer3.0.shortcut.1.num_batches_tracked', 'layer3.1.total_ops', 'layer3.1.total_params', 'layer3.1.left.0.weight', 'layer3.1.left.1.weight', 'layer3.1.left.1.bias', 'layer3.1.left.1.running_mean', 'layer3.1.left.1.running_var', 'layer3.1.left.1.num_batches_tracked', 'layer3.1.left.3.weight', 'layer3.1.left.4.weight', 'layer3.1.left.4.bias', 'layer3.1.left.4.running_mean', 'layer3.1.left.4.running_var', 'layer3.1.left.4.num_batches_tracked', 'layer4.0.total_ops', 'layer4.0.total_params', 'layer4.0.left.0.weight', 'layer4.0.left.1.weight', 'layer4.0.left.1.bias', 'layer4.0.left.1.running_mean', 'layer4.0.left.1.running_var', 'layer4.0.left.1.num_batches_tracked', 'layer4.0.left.3.weight', 'layer4.0.left.4.weight', 'layer4.0.left.4.bias', 'layer4.0.left.4.running_mean', 'layer4.0.left.4.running_var', 'layer4.0.left.4.num_batches_tracked', 'layer4.0.shortcut.0.weight', 'layer4.0.shortcut.1.weight', 'layer4.0.shortcut.1.bias', 'layer4.0.shortcut.1.running_mean', 'layer4.0.shortcut.1.running_var', 'layer4.0.shortcut.1.num_batches_tracked', 'layer4.1.total_ops', 'layer4.1.total_params', 'layer4.1.left.0.weight', 'layer4.1.left.1.weight', 'layer4.1.left.1.bias', 'layer4.1.left.1.running_mean', 'layer4.1.left.1.running_var', 'layer4.1.left.1.num_batches_tracked', 'layer4.1.left.3.weight', 'layer4.1.left.4.weight', 'layer4.1.left.4.bias', 'layer4.1.left.4.running_mean', 'layer4.1.left.4.running_var', 'layer4.1.left.4.num_batches_tracked', 'fc.weight', 'fc.bias'])\n",
      "missing keys: 0\n",
      "unexpected keys: 0\n",
      "num params: 11173962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 01:51:31.642000 1888113 site-packages/torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 13 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `ResNet18([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `ResNet18([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 13).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 40 of general pattern rewrite rules.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "torch_model = ResNet18(num_classes=10)\n",
    "dummy_input = (torch.randn(1, 3, 32, 32),)\n",
    "\n",
    "def export_onnx(model, dummy, path):\n",
    "    print(type(path), path)\n",
    "    state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "    if isinstance(state, dict):\n",
    "        print(\"checkpoint keys:\", state.keys())\n",
    "\n",
    "    # 常見包法處理\n",
    "    if isinstance(state, dict) and 'state_dict' in state:\n",
    "        state = state['state_dict']\n",
    "    elif isinstance(state, dict) and 'model' in state:\n",
    "        state = state['model']\n",
    "\n",
    "    new_state_dict = {}\n",
    "    for k, v in state.items():\n",
    "        name = k.replace(\"module.\", \"\")\n",
    "        if \"total_ops\" in name or \"total_params\" in name:\n",
    "            continue\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    missing, unexpected = model.load_state_dict(new_state_dict, strict=False)\n",
    "    print(\"missing keys:\", len(missing))\n",
    "    print(\"unexpected keys:\", len(unexpected))\n",
    "    print(\"num params:\", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "  # Todo : Export ONNX FILE\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy,\n",
    "        \"NM6131051_FP32.onnx\",\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        export_params=True,\n",
    "        opset_version=13,\n",
    "        external_data=False\n",
    "    )\n",
    "    pass\n",
    "if __name__ == \"__main__\":\n",
    "  # 提醒 : 記得先把 best_model.pth 上傳到 Content 資料夾\n",
    "  export_onnx(model=torch_model, dummy=dummy_input, path=\"best_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PnBfgSxfpUzD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calib will use input name: input\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np\n",
    "from PIL import Image\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import CalibrationDataReader\n",
    "\n",
    "CIFAR10_MEAN = np.array([0.4914, 0.4822, 0.4465], dtype=np.float32)\n",
    "CIFAR10_STD  = np.array([0.2470, 0.2435, 0.2616], dtype=np.float32)\n",
    "\n",
    "def preprocess_32x32(pil_img: Image.Image) -> np.ndarray:\n",
    "    arr = np.asarray(pil_img.convert(\"RGB\").resize((32, 32)), dtype=np.float32) / 255.0\n",
    "    arr = (arr - CIFAR10_MEAN) / CIFAR10_STD\n",
    "    return arr.transpose(2, 0, 1)[None, ...]  # (1,3,32,32)\n",
    "\n",
    "class CIFARLikeCalibReader(CalibrationDataReader):\n",
    "    def __init__(self, image_dir: str = None, input_name: str = \"input\",\n",
    "                 batch_size: int = 32, num_batches: int = 10):\n",
    "        self.input_name  = input_name\n",
    "        self.batch_size  = batch_size\n",
    "        self.num_batches = num_batches\n",
    "        self.paths = []\n",
    "        if image_dir and os.path.isdir(image_dir):\n",
    "            for f in os.listdir(image_dir):\n",
    "                if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
    "                    self.paths.append(os.path.join(image_dir, f))\n",
    "        self._mode_random = len(self.paths) == 0\n",
    "        self._pos = 0\n",
    "        self._emitted = 0\n",
    "\n",
    "    def get_next(self):\n",
    "        if self._emitted >= self.num_batches:\n",
    "            return None\n",
    "        if self._mode_random:\n",
    "            batch = np.random.randn(self.batch_size, 3, 32, 32).astype(np.float32)\n",
    "        else:\n",
    "            items = []\n",
    "            for _ in range(self.batch_size):\n",
    "                if self._pos >= len(self.paths):\n",
    "                    break\n",
    "                img = Image.open(self.paths[self._pos])\n",
    "                self._pos += 1\n",
    "                items.append(preprocess_32x32(img))\n",
    "            if not items:\n",
    "                return None\n",
    "            batch = np.concatenate(items, axis=0).astype(np.float32)\n",
    "        self._emitted += 1\n",
    "        return {self.input_name: batch}\n",
    "\n",
    "    def rewind(self):\n",
    "        self._pos = 0\n",
    "        self._emitted = 0\n",
    "\n",
    "FP32_MODEL = \"NM6131051_FP32.onnx\"\n",
    "INT8_MODEL = \"NM6131051_INT8.onnx\"\n",
    "\n",
    "\n",
    "_tmp = ort.InferenceSession(FP32_MODEL, providers=[\"CPUExecutionProvider\"])\n",
    "INPUT_NAME = _tmp.get_inputs()[0].name\n",
    "print(\"Calib will use input name:\", INPUT_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9_HL4D23phIN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please use QuantFormat.QDQ for activation type QInt8 and weight type QInt8. Or it will lead to bad performance on x64.\n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved INT8 model: NM6131051_INT8.onnx\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_static, QuantType, CalibrationMethod\n",
    "\n",
    "\n",
    "\n",
    "reader = CIFARLikeCalibReader(\n",
    "    image_dir=None,\n",
    "    input_name=INPUT_NAME,\n",
    "    batch_size=1,\n",
    "    num_batches=50\n",
    ")\n",
    "\n",
    "\n",
    "def quantize_to_int8(fp32_path, int8_path, reader, method=\"MinMax\"):\n",
    "    # Todo : quantize_static\n",
    "    quantize_static(\n",
    "            model_input=fp32_path,              \n",
    "            model_output=int8_path,            \n",
    "            calibration_data_reader=reader,   \n",
    "            quant_format=\"QOperator\", \n",
    "            per_channel=True,  \n",
    "            weight_type=QuantType.QInt8,       \n",
    "        )\n",
    "    print(\"Saved INT8 model:\", INT8_MODEL)\n",
    "\n",
    "quantize_to_int8(FP32_MODEL, INT8_MODEL, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VYZIE2Rdpj3G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Check] relative L2 diff FP32 vs INT8: 0.005730\n",
      "FP32 avg sec: 0.003746628761291504\n",
      "INT8 avg sec: 0.01162790298461914\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "def run(sess, x):\n",
    "    return sess.run(None, {sess.get_inputs()[0].name: x})[0]\n",
    "\n",
    "x_demo = np.random.randn(1,3,32,32).astype(np.float32)\n",
    "\n",
    "# Todo : build session function\n",
    "def build_session(model_path, providers):\n",
    "  session = ort.InferenceSession(model_path, providers=providers)\n",
    "  return session\n",
    "\n",
    "\n",
    "sess_fp32 = build_session(model_path=FP32_MODEL, providers=[\"CPUExecutionProvider\"])\n",
    "sess_int8 = build_session(model_path=INT8_MODEL, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "y_fp32 = run(sess_fp32, x_demo)\n",
    "y_int8 = run(sess_int8, x_demo)\n",
    "\n",
    "l2_rel = np.linalg.norm(y_fp32 - y_int8) / (np.linalg.norm(y_fp32) + 1e-12)\n",
    "print(f\"[Check] relative L2 diff FP32 vs INT8: {l2_rel:.6f}\")\n",
    "\n",
    "def bench(sess, x, n=50):\n",
    "    t0 = time.time()\n",
    "    for _ in range(n):\n",
    "        sess.run(None, {sess.get_inputs()[0].name: x})\n",
    "    return (time.time() - t0) / n\n",
    "\n",
    "print(\"FP32 avg sec:\", bench(sess_fp32, x_demo))\n",
    "print(\"INT8 avg sec:\", bench(sess_int8, x_demo))\n",
    "\n",
    "so = ort.SessionOptions()\n",
    "so.enable_profiling = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeFccLG1pehx"
   },
   "source": [
    "## Topic 2 : Gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qFNo0K7gvJqd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /home/ccching/miniconda3/lib/python3.13/site-packages (6.0.2)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (4.10.0)\n",
      "Requirement already satisfied: audioop-lts<1.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (0.2.2)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (1.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (0.124.0)\n",
      "Requirement already satisfied: ffmpy in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==2.0.1 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (2.0.1)\n",
      "Requirement already satisfied: groovy~=0.1 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (0.36.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (2.2.6)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (3.11.5)\n",
      "Requirement already satisfied: packaging in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<13.0,>=8.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (12.0.0)\n",
      "Requirement already satisfied: pydantic<=2.12.4,>=2.11.10 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (2.12.4)\n",
      "Requirement already satisfied: pydub in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (6.0.3)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (4.15.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio) (0.38.0)\n",
      "Requirement already satisfied: fsspec in /home/ccching/miniconda3/lib/python3.13/site-packages (from gradio-client==2.0.1->gradio) (2025.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ccching/miniconda3/lib/python3.13/site-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ccching/miniconda3/lib/python3.13/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /home/ccching/miniconda3/lib/python3.13/site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
      "Requirement already satisfied: certifi in /home/ccching/miniconda3/lib/python3.13/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ccching/miniconda3/lib/python3.13/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ccching/miniconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in /home/ccching/miniconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
      "Requirement already satisfied: requests in /home/ccching/miniconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ccching/miniconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ccching/miniconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ccching/miniconda3/lib/python3.13/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ccching/miniconda3/lib/python3.13/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ccching/miniconda3/lib/python3.13/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/ccching/miniconda3/lib/python3.13/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/ccching/miniconda3/lib/python3.13/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from typer<1.0,>=0.12->gradio) (1.5.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/ccching/miniconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ccching/miniconda3/lib/python3.13/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ccching/miniconda3/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ccching/miniconda3/lib/python3.13/site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ccching/miniconda3/lib/python3.13/site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_vE9Tel_tI76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccching/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://b78d9d257082c63e5c.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b78d9d257082c63e5c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "# ====== Config ======\n",
    "MODEL_PATH_INT8 = \"NM6131051_INT8.onnx\"   \n",
    "MODEL_PATH_FP32 = \"NM6131051_FP32.onnx\"    \n",
    "LABELS = ['plane','car','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "# CIFAR-10 Normalization Parameter\n",
    "CIFAR10_MEAN = np.array([0.4914, 0.4822, 0.4465], dtype=np.float32)\n",
    "CIFAR10_STD  = np.array([0.2470, 0.2435, 0.2616], dtype=np.float32)\n",
    "\n",
    "# ====== Utils ======\n",
    "def softmax_np(x: np.ndarray) -> np.ndarray:\n",
    "    x = x - np.max(x)\n",
    "    ex = np.exp(x)\n",
    "    return ex / np.sum(ex)\n",
    "\n",
    "# TODO : preprocess input image function\n",
    "def preprocess(image: Image.Image) -> np.ndarray:\n",
    "    \"\"\"輸入 PIL Image → (1,3,32,32) float32\"\"\"\n",
    "    if not isinstance(image, Image.Image):\n",
    "        raise ValueError(\"Plese Upload Image\")\n",
    "\n",
    "    img = image.resize((32, 32))\n",
    "    \n",
    "    arr = np.array(img).astype(np.float32) / 255.0\n",
    "    \n",
    "    arr = (arr - CIFAR10_MEAN) / CIFAR10_STD\n",
    "    \n",
    "    arr = arr.transpose(2, 0, 1)\n",
    "\n",
    "    arr = np.expand_dims(arr, axis=0)\n",
    "    return arr\n",
    "\n",
    "# ====== ONNX Sessions ======\n",
    "providers = ort.get_available_providers()\n",
    "\n",
    "sess_int8 = build_session(MODEL_PATH_INT8, providers=providers)\n",
    "in_int8  = sess_int8.get_inputs()[0].name\n",
    "out_int8 = sess_int8.get_outputs()[0].name\n",
    "\n",
    "\n",
    "try:\n",
    "    sess_fp32 = build_session(MODEL_PATH_FP32, providers=providers)\n",
    "    in_fp32  = sess_fp32.get_inputs()[0].name\n",
    "    out_fp32 = sess_fp32.get_outputs()[0].name\n",
    "    _fp32_err = \"\"\n",
    "except Exception as e:\n",
    "    sess_fp32, in_fp32, out_fp32 = None, None, None\n",
    "    _fp32_err = f\"[FP32 load failure] {type(e).__name__}: {e}\"\n",
    "\n",
    "# ====== Compare FP32 and INT8 ======\n",
    "# TODO : Compare FP32 and INT8\n",
    "def compare_fp32_int8(image: Image.Image):\n",
    "    if image is None:\n",
    "        return {}, {}, \"Please Upload Your Image。\"\n",
    "    if sess_fp32 is None:\n",
    "        return {}, {}, (_fp32_err or \"The FP32 model has not been provided, so a comparison cannot be made.\")\n",
    "\n",
    "    x = preprocess(image)\n",
    "\n",
    "    t0 = time.time()\n",
    "    res_fp32 = sess_fp32.run([out_fp32], {in_fp32: x})[0]\n",
    "    fp32_ms = (time.time() - t0) * 1000 \n",
    "\n",
    "    t0 = time.time()\n",
    "    res_int8 = sess_int8.run([out_int8], {in_int8: x})[0]\n",
    "    int8_ms = (time.time() - t0) * 1000 \n",
    "\n",
    "    p_fp32 = softmax_np(res_fp32[0])\n",
    "    p_int8 = softmax_np(res_int8[0])\n",
    "\n",
    "    def top3_map(p):\n",
    "        idx = np.argpartition(p, -3)[-3:]\n",
    "        idx = idx[np.argsort(p[idx])[::-1]]\n",
    "        return {LABELS[i]: float(p[i]) for i in idx}\n",
    "\n",
    "    top3_fp32 = top3_map(p_fp32)\n",
    "    top3_int8 = top3_map(p_int8)\n",
    "\n",
    "    summary = (\n",
    "        f\"FP32 inference time: {fp32_ms:.2f} ms\\n\"\n",
    "        f\"INT8 inference time: {int8_ms:.2f} ms\\n\"\n",
    "        f\"Speedup (FP32/INT8): {(fp32_ms / max(int8_ms, 1e-9)):.2f}×\"\n",
    "    )\n",
    "    return top3_fp32, top3_int8, summary\n",
    "\n",
    "# ====== Gradio UI ======\n",
    "# TODO : Building GUI Interface\n",
    "demo = gr.Interface(\n",
    "    fn = compare_fp32_int8,\n",
    "    inputs = gr.Image(type=\"pil\", label=\"Upload Image (CIFAR-10)\"),\n",
    "    outputs = [\n",
    "        gr.Label(num_top_classes=3, label=\"FP32 Prediction\"),\n",
    "        gr.Label(num_top_classes=3, label=\"INT8 Prediction\"),\n",
    "        gr.Textbox(label=\"Performance Comparison\")\n",
    "    ],\n",
    "    title = \"ResNet18 Quantization Demo: FP32 vs INT8\",\n",
    "    description = \"上傳一張圖片，比較全精度模型 (FP32) 與 量化模型 (INT8) 的預測結果與推論速度。\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # TODO : building a public web\n",
    "    # share=True 會產生公開的 Gradio 網址\n",
    "    demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOV/rmsswwtq7Dm7IE3hnwD",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
